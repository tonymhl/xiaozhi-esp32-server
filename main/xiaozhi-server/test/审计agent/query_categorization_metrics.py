"""
å®¡è®¡Agentå·¥å…·åˆ†ç±»å‡†ç¡®ç‡æµ‹è¯•è„šæœ¬

è¯¥è„šæœ¬ç”¨äºè¯„ä¼°æ¨¡å‹åœ¨é€‰æ‹©æ­£ç¡®å·¥å…·ï¼ˆregulation_search vs case_retrievalï¼‰æ—¶çš„æ€§èƒ½è¡¨ç°ã€‚
æ”¯æŒå¤šè½®æµ‹è¯•ã€å¹¶è¡Œæ‰§è¡Œã€è¯¦ç»†çš„metricsç»Ÿè®¡å’Œç»“æœå¯è§†åŒ–ã€‚
"""

import argparse
import asyncio
import csv
import os
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Tuple, Optional
from collections import defaultdict, Counter
import statistics

from fastmcp import Client
from openai import AsyncOpenAI


# æ ‡ç­¾åˆ°å·¥å…·åç§°çš„æ˜ å°„
LABEL_TO_TOOL = {
    "åˆ¶åº¦ç±»": "regulation_search",
    "å®ä¾‹ç±»": "case_retrieval",
}

# å·¥å…·åç§°åˆ°æ ‡ç­¾çš„åå‘æ˜ å°„
TOOL_TO_LABEL = {v: k for k, v in LABEL_TO_TOOL.items()}


class TestResult:
    """å•æ¬¡æµ‹è¯•ç»“æœ"""
    def __init__(self, question: str, true_label: str, predicted_tools: List[str], 
                 response_time: float = 0.0, raw_response: Optional[Any] = None):
        self.question = question
        self.true_label = true_label
        self.predicted_tools = predicted_tools
        self.response_time = response_time
        self.raw_response = raw_response
        
        # è®¡ç®—é¢„æµ‹æ ‡ç­¾
        self.predicted_label = self._get_predicted_label()
        self.is_correct = self.predicted_label == self.true_label
        
    def _get_predicted_label(self) -> Optional[str]:
        """æ ¹æ®é¢„æµ‹çš„å·¥å…·åˆ—è¡¨ç¡®å®šé¢„æµ‹æ ‡ç­¾"""
        if not self.predicted_tools:
            return None
        # å¦‚æœå¤šä¸ªå·¥å…·è¢«è°ƒç”¨ï¼Œå–ç¬¬ä¸€ä¸ª
        first_tool = self.predicted_tools[0]
        return TOOL_TO_LABEL.get(first_tool)
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            "question": self.question,
            "true_label": self.true_label,
            "predicted_tools": self.predicted_tools,
            "predicted_label": self.predicted_label,
            "is_correct": self.is_correct,
            "response_time": self.response_time,
        }


class MetricsCalculator:
    """æ€§èƒ½æŒ‡æ ‡è®¡ç®—å™¨ - åŒ…å«LLMå·¥å…·è°ƒç”¨ä¸“å±æŒ‡æ ‡"""
    
    def __init__(self, results: List[TestResult]):
        self.results = results
        self.labels = list(LABEL_TO_TOOL.keys())
        self.all_tools = list(LABEL_TO_TOOL.values())
        
    def calculate_confusion_matrix(self) -> Dict[str, Dict[str, int]]:
        """è®¡ç®—æ··æ·†çŸ©é˜µ"""
        matrix = {label: {pred: 0 for pred in self.labels + [None]} for label in self.labels}
        
        for result in self.results:
            if result.true_label in self.labels:
                matrix[result.true_label][result.predicted_label] = \
                    matrix[result.true_label].get(result.predicted_label, 0) + 1
        
        return matrix
    
    def calculate_metrics_per_class(self) -> Dict[str, Dict[str, float]]:
        """è®¡ç®—æ¯ä¸ªç±»åˆ«çš„precision, recall, f1"""
        metrics = {}
        
        for label in self.labels:
            tp = sum(1 for r in self.results if r.true_label == label and r.predicted_label == label)
            fp = sum(1 for r in self.results if r.true_label != label and r.predicted_label == label)
            fn = sum(1 for r in self.results if r.true_label == label and r.predicted_label != label)
            tn = sum(1 for r in self.results if r.true_label != label and r.predicted_label != label)
            
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
            
            support = sum(1 for r in self.results if r.true_label == label)
            
            metrics[label] = {
                "precision": precision,
                "recall": recall,
                "f1_score": f1,
                "support": support,
                "tp": tp,
                "fp": fp,
                "fn": fn,
                "tn": tn,
            }
        
        return metrics
    
    def calculate_tool_call_metrics(self) -> Dict[str, Any]:
        """è®¡ç®—LLMå·¥å…·è°ƒç”¨ä¸“å±æŒ‡æ ‡"""
        total = len(self.results)
        
        # 1. å·¥å…·è°ƒç”¨æˆåŠŸç‡ (Tool Call Success Rate)
        #    å®šä¹‰ï¼šæ¨¡å‹æˆåŠŸè°ƒç”¨äº†å·¥å…·ä¸”é€‰æ‹©æ­£ç¡®çš„æ¯”ä¾‹
        successful_calls = sum(1 for r in self.results if r.predicted_tools and r.is_correct)
        tool_call_success_rate = successful_calls / total if total > 0 else 0.0
        
        # 2. æ— å·¥å…·è°ƒç”¨ç‡ (No Tool Call Rate)
        #    å®šä¹‰ï¼šæ¨¡å‹æ²¡æœ‰è°ƒç”¨ä»»ä½•å·¥å…·çš„æ¯”ä¾‹
        no_tool_calls = sum(1 for r in self.results if not r.predicted_tools)
        no_tool_call_rate = no_tool_calls / total if total > 0 else 0.0
        
        # 3. å¤šå·¥å…·è°ƒç”¨ç‡ (Multiple Tool Calls Rate)
        #    å®šä¹‰ï¼šæ¨¡å‹ä¸€æ¬¡è°ƒç”¨äº†å¤šä¸ªå·¥å…·çš„æ¯”ä¾‹
        multiple_calls = sum(1 for r in self.results if len(r.predicted_tools) > 1)
        multiple_tool_call_rate = multiple_calls / total if total > 0 else 0.0
        
        # 4. å·¥å…·å¹»è§‰ç‡ (Tool Hallucination Rate)
        #    å®šä¹‰ï¼šæ¨¡å‹è°ƒç”¨äº†ä¸å­˜åœ¨çš„å·¥å…·çš„æ¯”ä¾‹
        hallucinated_calls = sum(1 for r in self.results 
                                if r.predicted_tools and 
                                any(tool not in self.all_tools for tool in r.predicted_tools))
        tool_hallucination_rate = hallucinated_calls / total if total > 0 else 0.0
        
        # 5. æœ‰æ•ˆå·¥å…·è°ƒç”¨ç‡ (Valid Tool Call Rate)
        #    å®šä¹‰ï¼šæ¨¡å‹è‡³å°‘è°ƒç”¨äº†ä¸€ä¸ªæœ‰æ•ˆå·¥å…·çš„æ¯”ä¾‹
        valid_calls = sum(1 for r in self.results 
                         if r.predicted_tools and 
                         any(tool in self.all_tools for tool in r.predicted_tools))
        valid_tool_call_rate = valid_calls / total if total > 0 else 0.0
        
        # 6. å•å·¥å…·è°ƒç”¨å‡†ç¡®ç‡ (Single Tool Call Accuracy)
        #    å®šä¹‰ï¼šåœ¨åªè°ƒç”¨ä¸€ä¸ªå·¥å…·çš„æƒ…å†µä¸‹çš„å‡†ç¡®ç‡
        single_calls = [r for r in self.results if len(r.predicted_tools) == 1]
        single_call_correct = sum(1 for r in single_calls if r.is_correct)
        single_tool_call_accuracy = single_call_correct / len(single_calls) if single_calls else 0.0
        
        # 7. å·¥å…·è°ƒç”¨åˆ†å¸ƒ
        tool_call_distribution = Counter()
        for result in self.results:
            for tool in result.predicted_tools:
                tool_call_distribution[tool] += 1
        
        # 8. å¹³å‡å·¥å…·è°ƒç”¨æ•°
        avg_tools_per_query = statistics.mean([len(r.predicted_tools) for r in self.results])
        
        # 9. å“åº”æ—¶é—´åˆ†ä½æ•°
        response_times = [r.response_time for r in self.results if r.response_time > 0]
        response_time_p50 = statistics.median(response_times) if response_times else 0.0
        response_time_p95 = statistics.quantiles(response_times, n=20)[18] if len(response_times) > 20 else (max(response_times) if response_times else 0.0)
        response_time_p99 = statistics.quantiles(response_times, n=100)[98] if len(response_times) > 100 else (max(response_times) if response_times else 0.0)
        
        return {
            "tool_call_success_rate": tool_call_success_rate,
            "no_tool_call_rate": no_tool_call_rate,
            "multiple_tool_call_rate": multiple_tool_call_rate,
            "tool_hallucination_rate": tool_hallucination_rate,
            "valid_tool_call_rate": valid_tool_call_rate,
            "single_tool_call_accuracy": single_tool_call_accuracy,
            "tool_call_distribution": dict(tool_call_distribution),
            "avg_tools_per_query": avg_tools_per_query,
            "response_time_p50": response_time_p50,
            "response_time_p95": response_time_p95,
            "response_time_p99": response_time_p99,
            "total_samples": total,
            "successful_calls": successful_calls,
            "no_tool_calls": no_tool_calls,
            "multiple_calls": multiple_calls,
            "hallucinated_calls": hallucinated_calls,
            "valid_calls": valid_calls,
            "single_calls_total": len(single_calls),
            "single_calls_correct": single_call_correct,
        }
    
    def calculate_overall_metrics(self) -> Dict[str, Any]:
        """è®¡ç®—æ•´ä½“æŒ‡æ ‡"""
        correct = sum(1 for r in self.results if r.is_correct)
        total = len(self.results)
        accuracy = correct / total if total > 0 else 0.0
        
        # è®¡ç®—macroå’Œweightedå¹³å‡
        class_metrics = self.calculate_metrics_per_class()
        
        macro_precision = statistics.mean([m["precision"] for m in class_metrics.values()])
        macro_recall = statistics.mean([m["recall"] for m in class_metrics.values()])
        macro_f1 = statistics.mean([m["f1_score"] for m in class_metrics.values()])
        
        # åŠ æƒå¹³å‡
        total_support = sum(m["support"] for m in class_metrics.values())
        weighted_precision = sum(m["precision"] * m["support"] for m in class_metrics.values()) / total_support if total_support > 0 else 0.0
        weighted_recall = sum(m["recall"] * m["support"] for m in class_metrics.values()) / total_support if total_support > 0 else 0.0
        weighted_f1 = sum(m["f1_score"] * m["support"] for m in class_metrics.values()) / total_support if total_support > 0 else 0.0
        
        # ç»Ÿè®¡æ— é¢„æµ‹çš„æƒ…å†µ
        no_prediction = sum(1 for r in self.results if r.predicted_label is None)
        
        # å“åº”æ—¶é—´ç»Ÿè®¡
        response_times = [r.response_time for r in self.results if r.response_time > 0]
        avg_response_time = statistics.mean(response_times) if response_times else 0.0
        min_response_time = min(response_times) if response_times else 0.0
        max_response_time = max(response_times) if response_times else 0.0
        
        return {
            "accuracy": accuracy,
            "correct_count": correct,
            "total_count": total,
            "no_prediction_count": no_prediction,
            "macro_precision": macro_precision,
            "macro_recall": macro_recall,
            "macro_f1": macro_f1,
            "weighted_precision": weighted_precision,
            "weighted_recall": weighted_recall,
            "weighted_f1": weighted_f1,
            "avg_response_time": avg_response_time,
            "min_response_time": min_response_time,
            "max_response_time": max_response_time,
        }


async def build_openai_tools_from_mcp(mcp_url: str) -> List[Dict[str, Any]]:
    """ä» MCP å®¢æˆ·ç«¯åŠ¨æ€è·å–å·¥å…·ï¼Œå¹¶è½¬æ¢ä¸º OpenAI Tools ç»“æ„"""
    async with Client(mcp_url) as mcp:
        await mcp.ping()
        tools = await mcp.list_tools()

    openai_tools: List[Dict[str, Any]] = []
    for tool in tools:
        openai_tools.append(
            {
                "type": "function",
                "function": {
                    "name": tool.name,
                    "description": tool.description,
                    "parameters": tool.inputSchema,
                },
            }
        )
    return openai_tools


async def run_single_query(
    aclient: AsyncOpenAI,
    model: str,
    question: str,
    temperature: float,
    max_tokens: int,
    tools: List[Dict[str, Any]],
) -> Tuple[List[str], float, Any]:
    """æ‰§è¡Œå•ä¸ªæŸ¥è¯¢ï¼Œè¿”å›å·¥å…·åç§°åˆ—è¡¨ã€å“åº”æ—¶é—´å’ŒåŸå§‹å“åº”"""
    
    now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å®¡è®¡ä¸çŸ¥è¯†æ£€ç´¢åŠ©æ‰‹ã€‚"
        f"\nç°åœ¨çš„æ—¶é—´æ˜¯ä¸Šæµ·æ—¶é—´: {now}"
        "\n\nä½ å¿…é¡»ä»ä»¥ä¸‹ä¸¤ä¸ªå·¥å…·ä¸­é€‰æ‹©ä¸€ä¸ªï¼ˆå¿…é¡»äºŒé€‰ä¸€ï¼‰ï¼š"
        "\n\nã€å·¥å…·é€‰æ‹©è§„åˆ™ - æ ¸å¿ƒåˆ¤æ–­é€»è¾‘ã€‘"
        "\n\n**ç¬¬ä¸€æ­¥ï¼šåˆ¤æ–­æŸ¥è¯¢çš„æ ¸å¿ƒæ„å›¾**"
        "\n\n1ï¸âƒ£ **å¦‚æœæŸ¥è¯¢çš„æ˜¯ã€Œè§„åˆ™ã€è¦æ±‚ã€æ ‡å‡†ã€å®šä¹‰ã€ç­‰è§„èŒƒæ€§å†…å®¹** â†’ regulation_search"
        "\n   å…³é”®è¯ï¼š\"å¦‚ä½•\"ã€\"æ€ä¹ˆ\"ã€\"ä»€ä¹ˆè¦æ±‚\"ã€\"ä»€ä¹ˆæ ‡å‡†\"ã€\"ä»€ä¹ˆæµç¨‹\"ã€"
        "\n           \"æœ‰å“ªäº›ç±»å‹\"ã€\"æœ‰å“ªäº›ç­‰çº§\"ã€\"è¦†ç›–èŒƒå›´\"ã€\"ç®¡ç†å‘¨æœŸ\"ã€\"èŒè´£\"ã€\"å®šä¹‰\""
        "\n   ç¤ºä¾‹ï¼š"
        "\n   âœ“ \"é—¨ç¦ç®¡ç†è¦æ±‚æ˜¯ä»€ä¹ˆ\" - è¯¢é—®ç®¡ç†è¦æ±‚"
        "\n   âœ“ \"æœ‰å“ªäº›åº”æ€¥é¢„æ¡ˆ\" - è¯¢é—®é¢„æ¡ˆç±»å‹/è¦†ç›–èŒƒå›´ï¼ˆä¸æ˜¯è¦å…·ä½“æ–‡æ¡£ï¼‰"
        "\n   âœ“ \"æ˜¯å¦æœ‰åº”æ€¥ç‰©èµ„åº“\" - è¯¢é—®æ˜¯å¦æœ‰è¿™ç§ç®¡ç†æœºåˆ¶"
        "\n   âœ“ \"åº”æ€¥ç»„ç»‡æ¶æ„å›¾æ˜¯ä»€ä¹ˆï¼Œå„è§’è‰²çš„èŒè´£æ˜¯å“ªäº›\" - è¯¢é—®èŒè´£åˆ†å·¥"
        "\n   âœ“ \"SH16å·¡æ£€è¦†ç›–èŒƒå›´ï¼Œå·¡æ£€é¢‘æ¬¡\" - è¯¢é—®è§„å®šçš„èŒƒå›´å’Œé¢‘æ¬¡"
        "\n   âœ“ \"ä¸‹å•çš„å‡†å¤‡å·¥ä½œæœ‰å“ªäº›\" - è¯¢é—®å·¥ä½œæµç¨‹è¦æ±‚"
        "\n   âœ“ \"è¯·æä¾›ä¾›åº”å•†ç›¸å…³ç®¡ç†è§„å®šæ–‡æ¡£\" - æ˜ç¡®è¦ç®¡ç†è§„å®š"
        "\n\n2ï¸âƒ£ **å¦‚æœæŸ¥è¯¢çš„æ˜¯ã€Œå…·ä½“æ–‡æ¡£ã€è®°å½•ã€è®¾å¤‡ã€æ•°æ®ã€ç­‰å®ä¾‹å†…å®¹** â†’ case_retrieval"
        "\n   å…³é”®è¯ï¼š\"è®¡åˆ’\"ã€\"è®°å½•\"ã€\"æ¸…å•\"ã€\"æŠ¥å‘Š\"ã€\"æµ‹è¯•\"ã€\"è¯ä¹¦\"ã€\"MOP\"ã€"
        "\n           \"è·¯çº¿\"ã€\"ç‚¹ä½\"ã€\"æƒé™åˆ—è¡¨\"ã€\"è¯·æä¾›XXè®°å½•/æ¸…å•/è¡¨æ ¼\"ã€\"çœ‹ä¸€ä¸‹\"ã€å…·ä½“å¹´ä»½/è®¾å¤‡+åè¯"
        "\n   ç¤ºä¾‹ï¼š"
        "\n   âœ“ \"2025å¹´åŸ¹è®­è®¡åˆ’\" - è¦å…·ä½“å¹´ä»½çš„è®¡åˆ’æ–‡æ¡£"
        "\n   âœ“ \"æ˜¯å¦æœ‰åŸ¹è®­è®°å½•\" - è¦å…·ä½“çš„è®°å½•"
        "\n   âœ“ \"è¯·æä¾›MOPæ¸…å•\" - è¦å…·ä½“çš„æ“ä½œæ‰‹å†Œæ¸…å•"
        "\n   âœ“ \"æµ¦æ±Ÿè¿ç»´ç»„ç»‡æ¶æ„å›¾\" - è¦å…·ä½“æœºæ„çš„æ¶æ„å›¾"
        "\n   âœ“ \"SH16å·¡æ£€è·¯çº¿\" - è¦å…·ä½“çš„è·¯çº¿ï¼ˆä¸æ˜¯è¦†ç›–èŒƒå›´ï¼‰"
        "\n   âœ“ \"è“„ç”µæ± æ”¾ç”µæµ‹è¯•çš„é¢‘ç‡\" - è¦ç»´æŠ¤è®¡åˆ’ä¸­çš„å…·ä½“å‚æ•°"
        "\n   âœ“ \"æœºæˆ¿å®‰å…¨å·¡é€»ç‚¹ä½\" - è¦å…·ä½“çš„ç‚¹ä½è¡¨"
        "\n   âœ“ \"BNPPæœºæŸœé—¨ç¦æƒé™\" - è¦å…·ä½“çš„æƒé™åˆ—è¡¨"
        "\n\n\nã€è¯¦ç»†åˆ¤æ–­è§„åˆ™ã€‘"
        "\n\nä¸€ã€**regulation_search (åˆ¶åº¦ç±»)** - æŸ¥è¯¢è§„èŒƒæ€§å†…å®¹"
        "\n   é€‚ç”¨åœºæ™¯ï¼š"
        "\n   â€¢ è¯¢é—®ã€Œå¦‚ä½•åšã€ã€ã€Œæ€ä¹ˆç®¡ç†ã€ã€ã€Œä»€ä¹ˆè¦æ±‚ã€ã€ã€Œä»€ä¹ˆæ ‡å‡†ã€"
        "\n   â€¢ è¯¢é—®ã€Œæœ‰å“ªäº›ç±»å‹/ç­‰çº§/åˆ†ç±»ã€ï¼ˆè¯¢é—®åˆ†ç±»ä½“ç³»ï¼Œä¸æ˜¯è¦å…·ä½“æ–‡æ¡£åˆ—è¡¨ï¼‰"
        "\n   â€¢ è¯¢é—®ã€Œæ˜¯å¦æœ‰XXç®¡ç†æœºåˆ¶/è§„å®šã€ï¼ˆè¯¢é—®æ˜¯å¦å­˜åœ¨è¿™ç§è§„å®šï¼‰"
        "\n   â€¢ è¯¢é—®ã€Œè¦†ç›–èŒƒå›´ã€ã€ã€ŒèŒè´£ã€ã€\"å®¡æ‰¹æµç¨‹\"ç­‰è§„å®šæ€§å†…å®¹"
        "\n   â€¢ æ˜ç¡®è¦æ±‚ã€Œç®¡ç†è§„å®šã€ã€ã€Œç®¡ç†åˆ¶åº¦ã€æ–‡æ¡£"
        "\n   â€¢ è¯¢é—®è§’è‰²èŒè´£ã€å®¡æ‰¹çŸ©é˜µã€åˆ†çº§åˆ†ç±»æ ‡å‡†ç­‰"
        "\n\näºŒã€**case_retrieval (å®ä¾‹ç±»)** - æŸ¥è¯¢å…·ä½“å®ä¾‹"
        "\n   é€‚ç”¨åœºæ™¯ï¼ˆæ»¡è¶³ä»¥ä¸‹ä»»ä¸€æ¡ä»¶ï¼‰ï¼š"
        "\n   â€¢ åŒ…å«å…·ä½“æ—¶é—´ï¼ˆå¹´/æœˆï¼‰+ åè¯ï¼ˆå¦‚\"2025å¹´XXè®¡åˆ’\"ã€\"æœ€è¿‘1å‘¨XX\"ï¼‰"
        "\n   â€¢ åŒ…å«å…·ä½“åœ°ç‚¹/è®¾å¤‡ + å…·ä½“åè¯ï¼ˆå¦‚\"XXæµ‹è¯•é¢‘ç‡\"ã€\"SH16XXè·¯çº¿\"ï¼‰"
        "\n   â€¢ æŸ¥è¯¢ã€ŒXXè®°å½•ã€ã€ã€ŒXXæ¸…å•ã€ã€ã€ŒXXæŠ¥å‘Šã€ã€\"XXè¯ä¹¦\"ã€\"XXè¡¨æ ¼/å·¥ä½œè¡¨\""
        "\n   â€¢ æŸ¥è¯¢ã€ŒMOPã€ã€ã€Œæ“ä½œæ‰‹å†Œã€ç­‰å…·ä½“æ“ä½œæ–‡æ¡£"
        "\n   â€¢ æŸ¥è¯¢ã€ŒXXè®¡åˆ’ã€ï¼ˆç»´æŠ¤è®¡åˆ’ã€åŸ¹è®­è®¡åˆ’ç­‰ï¼‰"
        "\n   â€¢ ä½¿ç”¨ã€Œè¯·æä¾›ã€+ éç®¡ç†è§„å®šçš„åè¯"
        "\n   â€¢ è¯¢é—®ã€Œæ˜¯å¦æœ‰XXè®°å½•/æ¸…å•ã€ï¼ˆè¦å…·ä½“çš„è®°å½•ï¼Œä¸æ˜¯è¯¢é—®æœºåˆ¶ï¼‰"
        "\n   â€¢ æŸ¥è¯¢å…·ä½“å‚æ•°/é…ç½®/ç‚¹ä½ï¼ˆå¦‚\"XXçš„é¢‘ç‡\"ã€\"XXæƒé™\"ã€\"XXç‚¹ä½\"ï¼‰"
        "\n\n\nã€å…³é”®åŒºåˆ†ç‚¹ã€‘"
        "\n\nğŸ” **\"æ˜¯å¦æœ‰...\"çš„åˆ¤æ–­ï¼š**"
        "\n   â€¢ \"æ˜¯å¦æœ‰åº”æ€¥ç‰©èµ„åº“\" â†’ regulation_searchï¼ˆè¯¢é—®æ˜¯å¦æœ‰è¿™ç§ç®¡ç†æœºåˆ¶ï¼‰"
        "\n   â€¢ \"æ˜¯å¦æœ‰åŸ¹è®­è®°å½•\" â†’ case_retrievalï¼ˆè¯¢é—®æ˜¯å¦å­˜åœ¨å…·ä½“çš„è®°å½•æ–‡æ¡£ï¼‰"
        "\n\nğŸ” **\"æœ‰å“ªäº›...\"çš„åˆ¤æ–­ï¼š**"
        "\n   â€¢ \"æœ‰å“ªäº›åº”æ€¥é¢„æ¡ˆ\" â†’ regulation_searchï¼ˆè¯¢é—®é¢„æ¡ˆç±»å‹/è¦†ç›–èŒƒå›´ï¼‰"
        "\n   â€¢ \"æœ‰å“ªäº›ç»´æŠ¤è®°å½•\" â†’ case_retrievalï¼ˆè¦å…·ä½“çš„è®°å½•åˆ—è¡¨ï¼‰"
        "\n\nğŸ” **å‚æ•°/é…ç½®çš„åˆ¤æ–­ï¼š**"
        "\n   â€¢ \"è“„ç”µæ± æ”¾ç”µæµ‹è¯•çš„é¢‘ç‡\" â†’ case_retrievalï¼ˆæŸ¥è¯¢å…·ä½“ç»´æŠ¤å‚æ•°ï¼‰"
        "\n   â€¢ \"åˆ—å¤´æŸœçš„å®‰å…¨ç®¡ç†é˜ˆå€¼æ˜¯ä»€ä¹ˆ\" â†’ regulation_searchï¼ˆæŸ¥è¯¢æ ‡å‡†/è§„å®šï¼‰"
        "\n\nğŸ” **æƒé™/ç‚¹ä½çš„åˆ¤æ–­ï¼š**"
        "\n   â€¢ \"BNPPæœºæŸœé—¨ç¦æƒé™\" â†’ case_retrievalï¼ˆæŸ¥è¯¢å…·ä½“æƒé™åˆ—è¡¨ï¼‰"
        "\n   â€¢ \"ä¸´æ—¶æ‹œè®¿äººå‘˜çš„é—¨ç¦æƒé™æ˜¯å¦‚ä½•ç®¡ç†çš„\" â†’ regulation_searchï¼ˆæŸ¥è¯¢ç®¡ç†æµç¨‹ï¼‰"
        "\n   â€¢ \"æœºæˆ¿å®‰å…¨å·¡é€»ç‚¹ä½\" â†’ case_retrievalï¼ˆæŸ¥è¯¢å…·ä½“ç‚¹ä½è¡¨ï¼‰"
        "\n\nğŸ” **åŒ…å«è®¾å¤‡ç¼–å·çš„åˆ¤æ–­ï¼š**"
        "\n   â€¢ \"SH16å·¡æ£€è¦†ç›–èŒƒå›´\" â†’ regulation_searchï¼ˆè¯¢é—®è§„å®šçš„èŒƒå›´ï¼‰"
        "\n   â€¢ \"SH16å·¡æ£€è·¯çº¿\" â†’ case_retrievalï¼ˆè¦å…·ä½“çš„è·¯çº¿å›¾/è·¯çº¿è¡¨ï¼‰"
        "\n\nğŸ” **æ¶æ„å›¾çš„åˆ¤æ–­ï¼š**"
        "\n   â€¢ \"åº”æ€¥ç»„ç»‡æ¶æ„å›¾æ˜¯ä»€ä¹ˆï¼Œå„è§’è‰²çš„èŒè´£æ˜¯å“ªäº›\" â†’ regulation_searchï¼ˆé‡ç‚¹åœ¨èŒè´£ï¼‰"
        "\n   â€¢ \"æµ¦æ±Ÿè¿ç»´ç»„ç»‡æ¶æ„å›¾\" â†’ case_retrievalï¼ˆè¦å…·ä½“æœºæ„çš„æ¶æ„å›¾æ–‡æ¡£ï¼‰"
        "\n\n\nã€å†³ç­–æµç¨‹ã€‘"
        "\n1. æ‰¾å‡ºæŸ¥è¯¢çš„æ ¸å¿ƒæ„å›¾å…³é”®è¯"
        "\n2. åˆ¤æ–­æ˜¯è¯¢é—®ã€Œè§„åˆ™/è¦æ±‚/æ ‡å‡†ã€è¿˜æ˜¯ã€Œå…·ä½“æ–‡æ¡£/æ•°æ®ã€"
        "\n3. å½“ä¸ç¡®å®šæ—¶ï¼Œçœ‹æ˜¯å¦åŒ…å«ï¼šå…·ä½“æ—¶é—´/åœ°ç‚¹/è®¾å¤‡ç¼–å· + å…·ä½“åè¯ï¼ˆè®°å½•/æ¸…å•/æŠ¥å‘Š/è®¡åˆ’/ç‚¹ä½/æƒé™ï¼‰â†’ case_retrieval"
        "\n4. å¦åˆ™ â†’ regulation_search"
        "\n\nè¯·ä»”ç»†åˆ†æç”¨æˆ·æŸ¥è¯¢ï¼Œå¿…é¡»é€‰æ‹©ä¸”åªèƒ½é€‰æ‹©ä¸€ä¸ªå·¥å…·ã€‚"
    )

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": question},
    ]

    start_time = asyncio.get_event_loop().time()
    try:
        response = await aclient.chat.completions.create(
            model=model,
            messages=messages,
            temperature=temperature,
            tools=tools,
            tool_choice="auto",
            max_completion_tokens=max_tokens,
        )
        end_time = asyncio.get_event_loop().time()
        response_time = end_time - start_time

        message = response.choices[0].message
        called_tool_names: List[str] = []
        if getattr(message, "tool_calls", None):
            for call in message.tool_calls:
                if getattr(call, "function", None) and getattr(call.function, "name", None):
                    called_tool_names.append(call.function.name)

        return called_tool_names, response_time, response
    except Exception as e:
        end_time = asyncio.get_event_loop().time()
        response_time = end_time - start_time
        print(f"Error processing question '{question}': {e}")
        return [], response_time, None


async def run_batch_test(
    aclient: AsyncOpenAI,
    model: str,
    test_cases: List[Tuple[str, str]],  # [(question, true_label), ...]
    temperature: float,
    max_tokens: int,
    tools: List[Dict[str, Any]],
    batch_size: int = 5,
) -> List[TestResult]:
    """æ‰¹é‡å¹¶è¡Œæµ‹è¯•"""
    results = []
    
    # åˆ†æ‰¹å¤„ç†ä»¥é¿å…è¿‡è½½
    for i in range(0, len(test_cases), batch_size):
        batch = test_cases[i:i + batch_size]
        tasks = []
        
        for question, true_label in batch:
            task = run_single_query(
                aclient, model, question, temperature, max_tokens, tools
            )
            tasks.append((question, true_label, task))
        
        # å¹¶è¡Œæ‰§è¡Œå½“å‰æ‰¹æ¬¡
        for question, true_label, task in tasks:
            predicted_tools, response_time, raw_response = await task
            result = TestResult(
                question=question,
                true_label=true_label,
                predicted_tools=predicted_tools,
                response_time=response_time,
                raw_response=raw_response,
            )
            results.append(result)
        
        print(f"å·²å®Œæˆ {min(i + batch_size, len(test_cases))}/{len(test_cases)} ä¸ªæµ‹è¯•")
    
    return results


def load_test_data(csv_path: str) -> List[Tuple[str, str]]:
    """ä»CSVæ–‡ä»¶åŠ è½½æµ‹è¯•æ•°æ®"""
    test_cases = []
    
    with open(csv_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        for row in reader:
            question = row.get('Q', '').strip()
            label = row.get('æ ‡ç­¾', '').strip()
            
            # è·³è¿‡ç©ºè¡Œæˆ–æ— æ•ˆæ•°æ®
            if question and label and label in LABEL_TO_TOOL:
                test_cases.append((question, label))
    
    return test_cases


def print_detailed_report(all_round_results: List[List[TestResult]], args):
    """æ‰“å°è¯¦ç»†çš„æµ‹è¯•æŠ¥å‘Š"""
    # æ”¶é›†æ‰€æœ‰è¾“å‡ºå†…å®¹
    report_lines = []
    
    def print_and_save(text=""):
        """åŒæ—¶æ‰“å°åˆ°æ§åˆ¶å°å’Œä¿å­˜åˆ°åˆ—è¡¨"""
        print(text)
        report_lines.append(text)
    
    print_and_save("\n" + "=" * 80)
    print_and_save("æµ‹è¯•æŠ¥å‘Š - å®¡è®¡Agentå·¥å…·åˆ†ç±»å‡†ç¡®ç‡è¯„ä¼°")
    print_and_save("=" * 80)
    print_and_save(f"\næµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print_and_save(f"æ¨¡å‹: {args.model}")
    print_and_save(f"æ¸©åº¦: {args.temperature}")
    print_and_save(f"æµ‹è¯•è½®æ•°: {args.rounds}")
    print_and_save(f"æ¯è½®æµ‹è¯•æ ·æœ¬æ•°: {len(all_round_results[0])}")
    
    # è®¡ç®—æ¯è½®çš„æŒ‡æ ‡
    print_and_save("\n" + "-" * 80)
    print_and_save("å„è½®æµ‹è¯•ç»“æœæ±‡æ€»")
    print_and_save("-" * 80)
    
    round_accuracies = []
    for round_idx, results in enumerate(all_round_results, 1):
        calculator = MetricsCalculator(results)
        overall = calculator.calculate_overall_metrics()
        round_accuracies.append(overall["accuracy"])
        
        print_and_save(f"\nç¬¬ {round_idx} è½®:")
        print_and_save(f"  å‡†ç¡®ç‡: {overall['accuracy']:.2%} ({overall['correct_count']}/{overall['total_count']})")
        print_and_save(f"  æ— é¢„æµ‹: {overall['no_prediction_count']} ä¸ª")
        print_and_save(f"  å¹³å‡å“åº”æ—¶é—´: {overall['avg_response_time']:.3f}ç§’")
    
    # è·¨è½®æ¬¡ç¨³å®šæ€§åˆ†æ
    if args.rounds > 1:
        print_and_save("\n" + "-" * 80)
        print_and_save("è·¨è½®æ¬¡ç¨³å®šæ€§åˆ†æ")
        print_and_save("-" * 80)
        print_and_save(f"å¹³å‡å‡†ç¡®ç‡: {statistics.mean(round_accuracies):.2%}")
        print_and_save(f"å‡†ç¡®ç‡æ ‡å‡†å·®: {statistics.stdev(round_accuracies) if len(round_accuracies) > 1 else 0:.4f}")
        print_and_save(f"å‡†ç¡®ç‡èŒƒå›´: [{min(round_accuracies):.2%}, {max(round_accuracies):.2%}]")
    
    # ä½¿ç”¨æœ€åä¸€è½®æˆ–æ‰€æœ‰è½®æ¬¡åˆå¹¶çš„ç»“æœè¿›è¡Œè¯¦ç»†åˆ†æ
    if args.aggregate_rounds:
        # åˆå¹¶æ‰€æœ‰è½®æ¬¡çš„ç»“æœ
        all_results = [r for round_results in all_round_results for r in round_results]
        print_and_save("\nä½¿ç”¨æ‰€æœ‰è½®æ¬¡çš„åˆå¹¶æ•°æ®è¿›è¡Œè¯¦ç»†åˆ†æ")
    else:
        # ä½¿ç”¨æœ€åä¸€è½®
        all_results = all_round_results[-1]
        print_and_save("\nä½¿ç”¨æœ€åä¸€è½®æ•°æ®è¿›è¡Œè¯¦ç»†åˆ†æ")
    
    calculator = MetricsCalculator(all_results)
    
    # æ•´ä½“æŒ‡æ ‡
    print_and_save("\n" + "-" * 80)
    print_and_save("æ•´ä½“æ€§èƒ½æŒ‡æ ‡")
    print_and_save("-" * 80)
    overall = calculator.calculate_overall_metrics()
    print_and_save(f"å‡†ç¡®ç‡ (Accuracy): {overall['accuracy']:.2%}")
    print_and_save(f"æ­£ç¡®é¢„æµ‹: {overall['correct_count']}/{overall['total_count']}")
    print_and_save(f"æ— é¢„æµ‹æ ·æœ¬: {overall['no_prediction_count']}")
    print_and_save(f"\nMacro å¹³å‡:")
    print_and_save(f"  Precision: {overall['macro_precision']:.2%}")
    print_and_save(f"  Recall: {overall['macro_recall']:.2%}")
    print_and_save(f"  F1-Score: {overall['macro_f1']:.2%}")
    print_and_save(f"\nWeighted å¹³å‡:")
    print_and_save(f"  Precision: {overall['weighted_precision']:.2%}")
    print_and_save(f"  Recall: {overall['weighted_recall']:.2%}")
    print_and_save(f"  F1-Score: {overall['weighted_f1']:.2%}")
    
    # LLMå·¥å…·è°ƒç”¨ä¸“å±æŒ‡æ ‡
    print_and_save("\n" + "-" * 80)
    print_and_save("LLMå·¥å…·è°ƒç”¨ä¸“å±æŒ‡æ ‡")
    print_and_save("-" * 80)
    tool_metrics = calculator.calculate_tool_call_metrics()
    print_and_save(f"\nå·¥å…·è°ƒç”¨è¡Œä¸ºåˆ†æ:")
    print_and_save(f"  å·¥å…·è°ƒç”¨æˆåŠŸç‡: {tool_metrics['tool_call_success_rate']:.2%} ({tool_metrics['successful_calls']}/{tool_metrics['total_samples']})")
    print_and_save(f"  æ— å·¥å…·è°ƒç”¨ç‡: {tool_metrics['no_tool_call_rate']:.2%} ({tool_metrics['no_tool_calls']}/{tool_metrics['total_samples']})")
    print_and_save(f"  å¤šå·¥å…·è°ƒç”¨ç‡: {tool_metrics['multiple_tool_call_rate']:.2%} ({tool_metrics['multiple_calls']}/{tool_metrics['total_samples']})")
    print_and_save(f"  å·¥å…·å¹»è§‰ç‡: {tool_metrics['tool_hallucination_rate']:.2%} ({tool_metrics['hallucinated_calls']}/{tool_metrics['total_samples']})")
    print_and_save(f"  æœ‰æ•ˆå·¥å…·è°ƒç”¨ç‡: {tool_metrics['valid_tool_call_rate']:.2%} ({tool_metrics['valid_calls']}/{tool_metrics['total_samples']})")
    
    print_and_save(f"\nå·¥å…·è°ƒç”¨è´¨é‡åˆ†æ:")
    print_and_save(f"  å•å·¥å…·è°ƒç”¨å‡†ç¡®ç‡: {tool_metrics['single_tool_call_accuracy']:.2%} ({tool_metrics['single_calls_correct']}/{tool_metrics['single_calls_total']})")
    print_and_save(f"  å¹³å‡å·¥å…·è°ƒç”¨æ•°/æŸ¥è¯¢: {tool_metrics['avg_tools_per_query']:.2f}")
    
    print_and_save(f"\nå·¥å…·è°ƒç”¨åˆ†å¸ƒ:")
    for tool, count in sorted(tool_metrics['tool_call_distribution'].items(), key=lambda x: x[1], reverse=True):
        percentage = count / tool_metrics['total_samples'] * 100
        print_and_save(f"  {tool}: {count} æ¬¡ ({percentage:.1f}%)")
    
    print_and_save(f"\nå“åº”æ—¶é—´åˆ†æ:")
    print_and_save(f"  å¹³å‡å“åº”æ—¶é—´: {overall['avg_response_time']:.3f}ç§’")
    print_and_save(f"  æœ€å°å“åº”æ—¶é—´: {overall['min_response_time']:.3f}ç§’")
    print_and_save(f"  æœ€å¤§å“åº”æ—¶é—´: {overall['max_response_time']:.3f}ç§’")
    print_and_save(f"  P50 (ä¸­ä½æ•°): {tool_metrics['response_time_p50']:.3f}ç§’")
    print_and_save(f"  P95: {tool_metrics['response_time_p95']:.3f}ç§’")
    print_and_save(f"  P99: {tool_metrics['response_time_p99']:.3f}ç§’")
    
    # æ¯ä¸ªç±»åˆ«çš„è¯¦ç»†æŒ‡æ ‡
    print_and_save("\n" + "-" * 80)
    print_and_save("å„ç±»åˆ«è¯¦ç»†æŒ‡æ ‡")
    print_and_save("-" * 80)
    class_metrics = calculator.calculate_metrics_per_class()
    
    for label, metrics in class_metrics.items():
        print_and_save(f"\nã€{label}ã€‘ (å¯¹åº”å·¥å…·: {LABEL_TO_TOOL[label]})")
        print_and_save(f"  æ ·æœ¬æ•° (Support): {metrics['support']}")
        print_and_save(f"  Precision: {metrics['precision']:.2%}")
        print_and_save(f"  Recall: {metrics['recall']:.2%}")
        print_and_save(f"  F1-Score: {metrics['f1_score']:.2%}")
        print_and_save(f"  æ··æ·†çŸ©é˜µç»Ÿè®¡: TP={metrics['tp']}, FP={metrics['fp']}, FN={metrics['fn']}, TN={metrics['tn']}")
    
    # æ··æ·†çŸ©é˜µ
    print_and_save("\n" + "-" * 80)
    print_and_save("æ··æ·†çŸ©é˜µ")
    print_and_save("-" * 80)
    confusion = calculator.calculate_confusion_matrix()
    
    # æ‰“å°è¡¨å¤´
    pred_labels = list(LABEL_TO_TOOL.keys()) + ["æ— é¢„æµ‹"]
    header = "çœŸå®\\é¢„æµ‹".ljust(12) + "".join([label.ljust(12) for label in pred_labels])
    print_and_save(header)
    print_and_save("-" * len(header))
    
    for true_label in LABEL_TO_TOOL.keys():
        row = true_label.ljust(12)
        for pred_label in LABEL_TO_TOOL.keys():
            count = confusion[true_label].get(pred_label, 0)
            row += str(count).ljust(12)
        # æ·»åŠ æ— é¢„æµ‹åˆ—
        count = confusion[true_label].get(None, 0)
        row += str(count).ljust(12)
        print_and_save(row)
    
    # é”™è¯¯æ¡ˆä¾‹åˆ†æ
    print_and_save("\n" + "-" * 80)
    print_and_save(f"é”™è¯¯æ¡ˆä¾‹åˆ†æ (å…¨éƒ¨ {len([r for r in all_results if not r.is_correct])} ä¸ª)")
    print_and_save("-" * 80)
    
    errors = [r for r in all_results if not r.is_correct]
    if errors:
        for idx, error in enumerate(errors, 1):
            print_and_save(f"\né”™è¯¯ {idx}:")
            print_and_save(f"  é—®é¢˜: {error.question}")
            print_and_save(f"  çœŸå®æ ‡ç­¾: {error.true_label}")
            print_and_save(f"  é¢„æµ‹å·¥å…·: {error.predicted_tools if error.predicted_tools else '(æ— )'}")
            print_and_save(f"  é¢„æµ‹æ ‡ç­¾: {error.predicted_label if error.predicted_label else '(æ— )'}")
    else:
        print_and_save("\nğŸ‰ æ²¡æœ‰é”™è¯¯æ¡ˆä¾‹ï¼")
    
    print_and_save("\n" + "=" * 80)
    
    # ä¿å­˜æŠ¥å‘Šåˆ°txtæ–‡ä»¶
    return report_lines


def save_results_to_json(all_round_results: List[List[TestResult]], output_path: str, args):
    """ä¿å­˜è¯¦ç»†ç»“æœåˆ°JSONæ–‡ä»¶"""
    output_data = {
        "meta": {
            "test_time": datetime.now().isoformat(),
            "model": args.model,
            "temperature": args.temperature,
            "rounds": args.rounds,
            "samples_per_round": len(all_round_results[0]),
        },
        "rounds": []
    }
    
    for round_idx, results in enumerate(all_round_results, 1):
        calculator = MetricsCalculator(results)
        round_data = {
            "round": round_idx,
            "overall_metrics": calculator.calculate_overall_metrics(),
            "tool_call_metrics": calculator.calculate_tool_call_metrics(),
            "class_metrics": calculator.calculate_metrics_per_class(),
            "confusion_matrix": calculator.calculate_confusion_matrix(),
            "results": [r.to_dict() for r in results],
        }
        output_data["rounds"].append(round_data)
    
    # å¦‚æœå¤šè½®ï¼Œè®¡ç®—æ±‡æ€»ç»Ÿè®¡
    if args.rounds > 1:
        accuracies = [r["overall_metrics"]["accuracy"] for r in output_data["rounds"]]
        output_data["aggregated_stats"] = {
            "mean_accuracy": statistics.mean(accuracies),
            "std_accuracy": statistics.stdev(accuracies) if len(accuracies) > 1 else 0,
            "min_accuracy": min(accuracies),
            "max_accuracy": max(accuracies),
        }
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)
    
    print(f"\nè¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_path}")


def parse_args() -> argparse.Namespace:
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    script_dir = Path(__file__).parent
    mcp_path = str((script_dir / "03_exp" / "audit_mcp.py").absolute())
    csv_path = str((script_dir / "å®¡è®¡QAâ€”â€”äººå·¥ç‰ˆ-å·¥ä½œè¡¨1.csv").absolute())
    
    parser = argparse.ArgumentParser(
        description="å®¡è®¡Agentå·¥å…·åˆ†ç±»å‡†ç¡®ç‡æµ‹è¯•",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    
    # æ•°æ®å’ŒMCPé…ç½®
    parser.add_argument("--csv-path", type=str, default=csv_path, 
                       help="æµ‹è¯•æ•°æ®CSVæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--mcp-url", type=str, default=os.getenv("MCP_URL", mcp_path),
                       help="MCPæœåŠ¡URLæˆ–è„šæœ¬è·¯å¾„")
    
    # æ¨¡å‹é…ç½®
    parser.add_argument("--provider", type=str, 
                       choices=["deepseek", "zhipu", "custom"],
                       default="deepseek",
                       help="APIæä¾›å•†ï¼šdeepseek, zhipu, æˆ– customï¼ˆè‡ªå®šä¹‰ï¼‰")
    parser.add_argument("--api-key", type=str, 
                       default=None,
                       help="APIå¯†é’¥ï¼ˆå¦‚æœä¸æŒ‡å®šï¼Œå°†ä½¿ç”¨providerçš„é»˜è®¤å€¼ï¼‰")
    parser.add_argument("--base-url", type=str, 
                       default=None,
                       help="APIåŸºç¡€URLï¼ˆå¦‚æœä¸æŒ‡å®šï¼Œå°†ä½¿ç”¨providerçš„é»˜è®¤å€¼ï¼‰")
    parser.add_argument("--model", type=str, 
                       default=None,
                       help="ä½¿ç”¨çš„æ¨¡å‹åç§°ï¼ˆå¦‚æœä¸æŒ‡å®šï¼Œå°†ä½¿ç”¨providerçš„é»˜è®¤å€¼ï¼‰")
    parser.add_argument("--temperature", type=float, default=0.3,
                       help="æ¨¡å‹æ¸©åº¦å‚æ•°")
    parser.add_argument("--max-tokens", type=int, default=1024,
                       help="æœ€å¤§ç”Ÿæˆtokenæ•°")
    parser.add_argument("--timeout", type=int, default=60,
                       help="APIè¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰")
    
    # æµ‹è¯•é…ç½®
    parser.add_argument("--rounds", type=int, default=1,
                       help="æµ‹è¯•è½®æ•°ï¼Œç”¨äºè¯„ä¼°ç¨³å®šæ€§")
    parser.add_argument("--batch-size", type=int, default=25,
                       help="å¹¶è¡Œæ‰¹å¤„ç†å¤§å°")
    parser.add_argument("--aggregate-rounds", action="store_true",
                       help="åˆå¹¶æ‰€æœ‰è½®æ¬¡çš„ç»“æœè¿›è¡Œåˆ†æï¼ˆè€Œä¸æ˜¯åªç”¨æœ€åä¸€è½®ï¼‰")
    
    # è¾“å‡ºé…ç½®
    parser.add_argument("--output-json", type=str, 
                       default=str(script_dir / "test_results.json"),
                       help="è¾“å‡ºJSONç»“æœæ–‡ä»¶è·¯å¾„")
    parser.add_argument("--no-save", action="store_true",
                       help="ä¸ä¿å­˜JSONç»“æœæ–‡ä»¶")
    
    return parser.parse_args()


async def amain():
    """ä¸»å¼‚æ­¥å‡½æ•°"""
    args = parse_args()
    
    # æ ¹æ®providerè®¾ç½®é»˜è®¤å€¼
    provider_configs = {
        "deepseek": {
            "api_key": "av7b4VoBryCMu3hZ9",
            "base_url": "https://deepseek.gds-services.com/v1",
            "model": "deepseek-ai/DeepSeek-V3"
        },
        "zhipu": {
            "api_key": "29ada920c778a1c8341267040b0e31f0.WYoDydHDDDbmsqAx",
            "base_url": "https://open.bigmodel.cn/api/paas/v4",
            "model": "glm-4-flash"
        }
    }
    
    # åº”ç”¨é»˜è®¤é…ç½®
    if args.provider in provider_configs:
        config = provider_configs[args.provider]
        if not args.api_key:
            args.api_key = config["api_key"]
        if not args.base_url:
            args.base_url = config["base_url"]
        if not args.model:
            args.model = config["model"]
    else:
        # customæ¨¡å¼ï¼Œå¿…é¡»æ‰‹åŠ¨æŒ‡å®šæ‰€æœ‰å‚æ•°
        if not args.api_key or not args.base_url or not args.model:
            raise SystemExit("é”™è¯¯: ä½¿ç”¨customæ¨¡å¼æ—¶ï¼Œå¿…é¡»æŒ‡å®š--api-keyã€--base-urlå’Œ--modelå‚æ•°")
    
    print("=" * 80)
    print("å®¡è®¡Agentå·¥å…·åˆ†ç±»å‡†ç¡®ç‡æµ‹è¯•")
    print("=" * 80)
    print(f"\nAPIæä¾›å•†: {args.provider}")
    print(f"æ¨¡å‹: {args.model}")
    print(f"\nåŠ è½½æµ‹è¯•æ•°æ®: {args.csv_path}")
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_cases = load_test_data(args.csv_path)
    
    if not test_cases:
        raise SystemExit("é”™è¯¯: æœªèƒ½ä»CSVæ–‡ä»¶ä¸­åŠ è½½æœ‰æ•ˆçš„æµ‹è¯•æ•°æ®")
    
    print(f"æˆåŠŸåŠ è½½ {len(test_cases)} ä¸ªæµ‹è¯•æ ·æœ¬")
    
    # ç»Ÿè®¡æ ‡ç­¾åˆ†å¸ƒ
    label_counts = Counter([label for _, label in test_cases])
    print(f"æ ‡ç­¾åˆ†å¸ƒ: {dict(label_counts)}")
    
    # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
    client_kwargs = {
        "api_key": args.api_key or "",
        "base_url": args.base_url,
        "timeout": args.timeout,
    }
    aclient = AsyncOpenAI(**client_kwargs)
    
    # ä»MCPè·å–å·¥å…·åˆ—è¡¨
    print(f"\nè¿æ¥MCPæœåŠ¡: {args.mcp_url}")
    tools = await build_openai_tools_from_mcp(args.mcp_url)
    print(f"æˆåŠŸåŠ è½½ {len(tools)} ä¸ªå·¥å…·: {[t['function']['name'] for t in tools]}")
    
    # æ‰§è¡Œå¤šè½®æµ‹è¯•
    all_round_results = []
    
    for round_idx in range(1, args.rounds + 1):
        print(f"\n{'=' * 80}")
        print(f"å¼€å§‹ç¬¬ {round_idx}/{args.rounds} è½®æµ‹è¯•")
        print(f"{'=' * 80}")
        
        results = await run_batch_test(
            aclient=aclient,
            model=args.model,
            test_cases=test_cases,
            temperature=args.temperature,
            max_tokens=args.max_tokens,
            tools=tools,
            batch_size=args.batch_size,
        )
        
        all_round_results.append(results)
        print(f"ç¬¬ {round_idx} è½®æµ‹è¯•å®Œæˆ")
    
    # æ‰“å°è¯¦ç»†æŠ¥å‘Šå¹¶è·å–æŠ¥å‘Šå†…å®¹
    report_lines = print_detailed_report(all_round_results, args)
    
    # ä¿å­˜ç»“æœ
    if not args.no_save:
        save_results_to_json(all_round_results, args.output_json, args)
        
        # ä¿å­˜æ–‡æœ¬æŠ¥å‘Š
        txt_output = args.output_json.replace('.json', '.txt')
        with open(txt_output, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))
        print(f"æ–‡æœ¬æŠ¥å‘Šå·²ä¿å­˜åˆ°: {txt_output}")
    
    print("\næµ‹è¯•å®Œæˆ!")


def main():
    """ä¸»å‡½æ•°å…¥å£"""
    asyncio.run(amain())


if __name__ == "__main__":
    main()
